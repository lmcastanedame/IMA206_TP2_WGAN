{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "486ac486",
      "metadata": {
        "id": "486ac486"
      },
      "source": [
        "# Exercice 1: Learning a WGAN for synthetic 2-dimensional datasets\n",
        "\n",
        "<br/><br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70126ac5",
      "metadata": {
        "id": "70126ac5"
      },
      "source": [
        "This practical session contains\n",
        "- \"QUESTION\" fields in the text, that you should answer\n",
        "- blocks of code that you should complete at every region marked with ### ... ###\n",
        "\n",
        "We advise you to open the notebook on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9745878b",
      "metadata": {
        "id": "9745878b"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1127a8",
      "metadata": {
        "id": "de1127a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    dtype = torch.cuda.FloatTensor\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    dtype = torch.FloatTensor\n",
        "\n",
        "# If you don't want to bother with the device, stay on cpu:\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c8f5f7",
      "metadata": {
        "id": "d4c8f5f7"
      },
      "source": [
        "### Target Measure\n",
        "\n",
        "In the following cell, we define the discrete target measure $\\nu$ that will serve as dataset for this practical session.\n",
        "\n",
        "The variable `xgrid` contains a grid of points that will be useful below to display the discriminators along training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff21cfc",
      "metadata": {
        "id": "aff21cfc"
      },
      "outputs": [],
      "source": [
        "d = 2   # dimension of the data points\n",
        "\n",
        "# First dataset with 6 points\n",
        "n = 6\n",
        "y = torch.zeros((n,d), device=device)\n",
        "y[0, 0] = 0.9\n",
        "y[0, 1] = 0.2\n",
        "y[1, 0] = 0.75\n",
        "y[1, 1] = 0.8\n",
        "y[2, 0] = 0.3\n",
        "y[2, 1] = 0.4\n",
        "y[3, 0] = 0.4\n",
        "y[3, 1] = 0.7\n",
        "y[4, 0] = 0.45\n",
        "y[4, 1] = 0.75\n",
        "y[5, 0] = 0.7\n",
        "y[5, 1] = 0.5\n",
        "\n",
        "# # Second dataset with 100 points\n",
        "# n = 100\n",
        "# t = torch.pi*torch.linspace(-.2,1.2,n)\n",
        "# y = .05*torch.randn((n,d))\n",
        "# y[:,0] += torch.cos(t)\n",
        "# y[:,1] += torch.sin(2*t)\n",
        "# y = .5 + .3*y\n",
        "# y = y.to(device)\n",
        "\n",
        "# Define masses (empirical measure on the data points)\n",
        "nu = torch.ones(n, device=device)/n\n",
        "\n",
        "# generate grid for plotting purpose\n",
        "nr,nc = 256,256\n",
        "extent = ((-0.5/nc, 1-0.5/nc, 1-0.5/nr, -0.5/nr))\n",
        "xs = torch.linspace(0, 1, steps=nr)\n",
        "ys = torch.linspace(0, 1, steps=nc)\n",
        "xm, ym = torch.meshgrid(xs, ys, indexing='ij')\n",
        "xm = xm.T\n",
        "ym = ym.T\n",
        "xgrid = torch.cat((xm.reshape(nr*nc,1),ym.reshape(nr*nc,1)),1).to(device)\n",
        "\n",
        "# Plot data points\n",
        "fig = plt.figure(dpi=100)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.imshow(np.zeros((nr,nc)),cmap = 'Oranges', extent=extent) # background\n",
        "plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7252b7bb",
      "metadata": {
        "id": "7252b7bb"
      },
      "source": [
        "## Define Generator architecture\n",
        "\n",
        "QUESTION: Examine the layers and parameters of the following generative network.\n",
        "\n",
        "Write a short description of the architecture of this network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6b87c0",
      "metadata": {
        "id": "cf6b87c0"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_in, n_out, n_hid=10, nlayers=3, device=torch.device(\"cpu\")):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.n_hid = n_hid\n",
        "        self.nlayers = nlayers\n",
        "        self.hidden = nn.ModuleList()\n",
        "\n",
        "        for n in range(nlayers):\n",
        "            n_in_t = n_in if n==0 else n_hid\n",
        "            self.hidden.append(nn.Sequential(\n",
        "            nn.Linear(n_in_t, n_hid),\n",
        "            nn.ELU(1)\n",
        "        ).to(device))\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(n_hid, n_out),\n",
        "            nn.Sigmoid()\n",
        "        ).to(device)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for n in range(self.nlayers):\n",
        "            x = self.hidden[n](x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight, 1.0)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b16a0f",
      "metadata": {
        "id": "a5b16a0f"
      },
      "source": [
        "QUESTION: Plot one initial configuration of the generator (draw a batch of generated points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5c34ca",
      "metadata": {
        "id": "ef5c34ca"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)  # initialize random seed for reproducibility\n",
        "\n",
        "n_in = 10    # dimension of the input noise\n",
        "b = 100      # batch size\n",
        "\n",
        "# Initialize generator\n",
        "G = Generator(n_in=n_in, n_out=d, n_hid=100, nlayers=3, device=device)\n",
        "\n",
        "# Draw a batch x of generated points\n",
        "#    Input noise z : standard normal with shape (b, n_in)\n",
        "\n",
        "### ... ###\n",
        "\n",
        "# Plot\n",
        "xd = x.detach()\n",
        "fig = plt.figure(dpi=100)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.imshow(np.zeros((nr,nc)),cmap = 'Oranges', extent=extent) # background\n",
        "plt.scatter(xd[:, 0].cpu(), xd[:,1].cpu(),c='deepskyblue',alpha=.5)\n",
        "plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5722cf",
      "metadata": {
        "id": "2e5722cf"
      },
      "source": [
        "## Define Discriminator Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5b7da1",
      "metadata": {
        "id": "3b5b7da1"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, n_in, n_hid=10):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.n_hid = n_hid\n",
        "    self.n_in = n_in\n",
        "\n",
        "    self.fc1 = nn.Linear(n_in, n_hid)\n",
        "    self.fc2 = nn.Linear(n_hid, n_hid)\n",
        "    self.fc3 = nn.Linear(n_hid, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = nn.LeakyReLU(negative_slope=0.2)(self.fc1(x))\n",
        "    y = nn.LeakyReLU(negative_slope=0.2)(self.fc2(y))\n",
        "    y = self.fc3(y)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f13622",
      "metadata": {
        "id": "c3f13622"
      },
      "source": [
        "## Discriminator training with Weight clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed78b86",
      "metadata": {
        "id": "eed78b86"
      },
      "source": [
        "QUESTION: For a fixed generator, train the discriminator with WGAN loss and weight clipping.\n",
        "\n",
        "Try changing the clip_value. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c224ab56",
      "metadata": {
        "scrolled": false,
        "id": "c224ab56"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
        "\n",
        "# fix one generator\n",
        "G = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "\n",
        "# parameters for discriminator optimization\n",
        "lrdisc = 0.002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "niterD=1000\n",
        "\n",
        "clip_value = .1\n",
        "\n",
        "D = Discriminator(n_in=d, n_hid=10).to(device)\n",
        "optimD = optim.Adam(D.parameters(), lr=lrdisc, betas=(beta_1, beta_2))\n",
        "\n",
        "iter_display = 100  # display current configuration each iter_display iteration\n",
        "\n",
        "# Lists to keep track of progress\n",
        "Dlosses = []\n",
        "\n",
        "for iter in range(0,niterD):\n",
        "\n",
        "    ### UPDATE OF D ###\n",
        "    optimD.zero_grad()\n",
        "\n",
        "    ### ... ###\n",
        "\n",
        "    Dloss =  ### ... ###\n",
        "\n",
        "    ### ... ###\n",
        "\n",
        "    optimD.step()\n",
        "\n",
        "    ### ... Weight Clipping ... ###\n",
        "\n",
        "    ### SAVE LOSS ###\n",
        "    Dlosst = Dloss.item()\n",
        "    Dlosses.append(-Dlosst)\n",
        "\n",
        "    if(iter%iter_display == 0):\n",
        "        print('[%d/%d], %f' % (iter, niterD, Dlosst))\n",
        "        Dxgrid = D(xgrid).detach().cpu().numpy().reshape(nr,nc)\n",
        "        x = G(z)\n",
        "        xd = x.detach().squeeze(1)\n",
        "        strtitle = 'Iter '+str(iter)\n",
        "        fig = plt.figure(dpi=100)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(Dxgrid,cmap = 'Oranges', extent=extent)  # discriminator\n",
        "        plt.scatter(xd[:, 0].cpu(), xd[:,1].cpu(),c='deepskyblue',alpha=.5)\n",
        "        plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "        plt.title(strtitle)\n",
        "        plt.show()\n",
        "\n",
        "plt.plot(Dlosses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b650177",
      "metadata": {
        "id": "9b650177"
      },
      "source": [
        "## Estimate the Lipschitz constant of the discriminator\n",
        "\n",
        "The following function computes a lower bound of the Lipschitz constant of $D$ on points that are interpolated between $x$ and $y$.\n",
        "\n",
        "NB: If $x$ and $y$ do not have the same number of points, we discard the last points.\n",
        "In comment, we give an alternative code that allows to compute all segments $[x_i, y_j]$ (but it is, of course, slower that just computing segments $[x_i, y_i]$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8d5e11",
      "metadata": {
        "id": "5b8d5e11"
      },
      "outputs": [],
      "source": [
        "def lipconstant(D,x,y):\n",
        "    b = x.shape[0]\n",
        "    n = y.shape[0]\n",
        "    # shrink vectors if they are too large\n",
        "    if n>b:\n",
        "        y = y[0:b,:]\n",
        "        n = b\n",
        "    else:\n",
        "        x = x[0:n,:]\n",
        "        b = n\n",
        "    # compute interpolated points\n",
        "    alpha = torch.rand((b,1),device=device)\n",
        "    interp = alpha * y + (1 - alpha) * x\n",
        "    interp.requires_grad_()\n",
        "\n",
        "    # Calculate probability of interpolated examples\n",
        "    Di = D(interp).view(-1)\n",
        "\n",
        "    # Calculate gradients of probabilities with respect to examples\n",
        "    gradout = torch.ones(Di.size()).to(device)\n",
        "    gradients = torch.autograd.grad(outputs=Di, inputs=interp, grad_outputs=gradout,\n",
        "       create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    # Derivatives of the gradient close to 0 can cause problems because of\n",
        "    # the square root, so manually calculate norm and add epsilon\n",
        "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1))\n",
        "\n",
        "    # Return gradient penalty\n",
        "    return torch.mean(gradients_norm)\n",
        "\n",
        "#     # Calculate interpolation\n",
        "#     b = x.shape[0]\n",
        "#     n = y.shape[0]\n",
        "#     alpha = torch.rand((b,n,1),device=device)\n",
        "#     interp = (alpha * y[None,:,:] + (1 - alpha) * x[:,None,:]).flatten(end_dim=1)\n",
        "#     interp.requires_grad_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f41b802",
      "metadata": {
        "id": "1f41b802"
      },
      "source": [
        "\n",
        "QUESTION: Use this function to examine the Lipschitz constant of the final discriminator obtained above with weight clipping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "801dcf55",
      "metadata": {
        "id": "801dcf55"
      },
      "outputs": [],
      "source": [
        "### ... ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5e6af58",
      "metadata": {
        "id": "c5e6af58"
      },
      "source": [
        "## Gradient Penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbf63e3",
      "metadata": {
        "id": "ecbf63e3"
      },
      "source": [
        "QUESTION: Implement a function computing the gradient penalty of a discriminator $D$ on points $X$ that are randomly interpolated between $x$ and $y$:\n",
        "$$ GP(D) = \\mathbb{E}\\left[ \\Big(\\|\\nabla D(X)\\| - 1 \\Big)^2 \\right] .$$\n",
        "In pratice you will use\n",
        "$$\\|\\nabla D(X)\\| \\approx \\sqrt{\\|\\nabla D (X)\\|_2^2 + 10^{-12} } $$\n",
        "to avoid numerical instability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055df621",
      "metadata": {
        "id": "055df621"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(D,x,y):\n",
        "    b = x.shape[0]\n",
        "    n = y.shape[0]\n",
        "    # shrink vectors if they are too large\n",
        "    if n>b:\n",
        "        y = y[0:b,:]\n",
        "        n = b\n",
        "    else:\n",
        "        x = x[0:n,:]\n",
        "        b = n\n",
        "    # compute interpolated points\n",
        "    alpha = torch.rand((b,1),device=device)\n",
        "    interp = (alpha * y + (1 - alpha) * x)\n",
        "    interp.requires_grad_()\n",
        "\n",
        "#     # Calculate interpolation\n",
        "#     b = x.shape[0]\n",
        "#     n = y.shape[0]\n",
        "#     alpha = torch.rand((b,n,1),device=device)\n",
        "#     interp = (alpha * y[None,:,:] + (1 - alpha) * x[:,None,:]).flatten(end_dim=1)\n",
        "#     interp.requires_grad_()\n",
        "\n",
        "    # Calculate probability of interpolated examples\n",
        "    Di = D(interp).view(-1)\n",
        "\n",
        "    # Calculate gradients of probabilities with respect to examples\n",
        "    gradout = torch.ones(Di.size()).to(device)\n",
        "    gradients = torch.autograd.grad(outputs=Di, inputs=interp, grad_outputs=gradout,\n",
        "       create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    # Derivatives of the gradient close to 0 can cause problems because of\n",
        "    # the square root, so manually calculate norm and add epsilon\n",
        "    gradients_norm = ### ... ###\n",
        "\n",
        "    # Return gradient penalty\n",
        "    return ((gradients_norm - 1) ** 2).mean()\n",
        "#     return ((gradients_norm - 1) ** 2*(gradients_norm>1)).mean()\n",
        "\n",
        "print(gradient_penalty(D,x,y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2d104c",
      "metadata": {
        "id": "1c2d104c"
      },
      "source": [
        "## Discriminator training with Gradient Penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24068817",
      "metadata": {
        "id": "24068817"
      },
      "source": [
        "QUESTION: Complete the following code at the blocks ###...###.\n",
        "\n",
        "Adjust the weight of the gradient penalty (parameter `gpw`) to get a Lipschitz constant $\\approx 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e4b289",
      "metadata": {
        "scrolled": false,
        "id": "80e4b289"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
        "\n",
        "# Initialize generators and discriminators\n",
        "G = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "D = Discriminator(n_in=d, n_hid=10).to(device)\n",
        "\n",
        "# parameters for training\n",
        "lrdisc = 0.002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "niterD=1000\n",
        "gpw = 10       # parameter for gradient penalty\n",
        "\n",
        "optimD = optim.Adam(D.parameters(), lr=lrdisc, betas=(beta_1, beta_2))\n",
        "\n",
        "\n",
        "iter_display = 100  # display current configuration each iter_display iteration\n",
        "\n",
        "# Lists to keep track of progress\n",
        "Dlosses = []\n",
        "\n",
        "for iter in range(0,niterD):\n",
        "\n",
        "    ### UPDATE OF D ###\n",
        "\n",
        "    ### ... ###\n",
        "\n",
        "    ### SAVE LOSS ###\n",
        "    Dlosst = Dloss.item()\n",
        "    Dlosses.append(-Dlosst)\n",
        "\n",
        "    if(iter%iter_display == 0):\n",
        "        print('[%d/%d], Dloss=%.4f, Lip(D)=%.4f' % (iter, niterD, Dlosst,lipconstant(D,x,y).item()))\n",
        "        Dxgrid = D(xgrid).detach().cpu().numpy().reshape(nr,nc)\n",
        "        x = G(z)\n",
        "        xd = x.detach().squeeze(1)\n",
        "        strtitle = 'Iter '+str(iter)\n",
        "        fig = plt.figure(dpi=100)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(Dxgrid,cmap = 'Oranges', extent=extent)  # discriminator\n",
        "        plt.scatter(xd[:, 0].cpu(), xd[:,1].cpu(),c='deepskyblue',alpha=.5)\n",
        "        plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "        plt.title(strtitle)\n",
        "        plt.show()\n",
        "\n",
        "print('Final estimated Lipschitz constant = ',lipconstant(D,x,y).item())\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "plt.plot(Dlosses)\n",
        "plt.title('Discriminator loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02719b55",
      "metadata": {
        "id": "02719b55"
      },
      "source": [
        "## WGAN-GP training\n",
        "\n",
        "QUESTION: Complete the following code in order to train simultaneously the generator and discriminator.\n",
        "\n",
        "You will alternate `niterD` iterations on the discriminator and `niterG` iterations of the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ab2de9",
      "metadata": {
        "scrolled": false,
        "id": "62ab2de9"
      },
      "outputs": [],
      "source": [
        "## parameters for training\n",
        "n_epochs = 100\n",
        "niterD=1000\n",
        "niterG=10\n",
        "gpw = .1\n",
        "\n",
        "lr = 0.002      # learning rate for generator\n",
        "lrdisc = 0.002  # learning rate for discriminator\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "\n",
        "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
        "\n",
        "# Initialize generators and discriminators\n",
        "G = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "optimG = optim.Adam(G.parameters(), lr=lr)\n",
        "\n",
        "D = Discriminator(n_in=d, n_hid=10).to(device)\n",
        "optimD = optim.Adam(D.parameters(), lr=lrdisc, betas=(beta_1, beta_2))\n",
        "\n",
        "\n",
        "Glosses = []\n",
        "Dlosses = []\n",
        "\n",
        "iter_display = 5\n",
        "\n",
        "# Main loop\n",
        "for epoch in range(1,n_epochs):\n",
        "\n",
        "    ############################\n",
        "    ### Train discriminator (niterD iterations)\n",
        "    ############################\n",
        "\n",
        "    for iter in range(0,niterD):\n",
        "        ### ... ###\n",
        "\n",
        "    ############################\n",
        "    ### Train generator (niterG iterations)\n",
        "    ############################\n",
        "    for iter in range(0,niterG):\n",
        "        ### ... ###\n",
        "\n",
        "    # Output training stats\n",
        "    print('[%d/%d] \\tLoss_D: %.4f\\tLoss_G: %.4f\\tLip(D)=%.4f'\n",
        "      % (epoch, n_epochs, Dloss.item(), Gloss.item(),lipconstant(D,x,y).item()))\n",
        "    Glosses.append(Gloss.item())\n",
        "    Dlosses.append(Dloss.item())\n",
        "\n",
        "    if(epoch % iter_display == 0):\n",
        "        Dxgrid = D(xgrid).detach().cpu().numpy().reshape(nr,nc)\n",
        "        z = torch.randn(b, n_in, device=device)\n",
        "        x = G(z)\n",
        "        xd = x.detach().squeeze(1)\n",
        "        strtitle = 'Epoch '+str(epoch)\n",
        "        fig = plt.figure(dpi=100)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(Dxgrid,cmap = 'Oranges', extent=extent)\n",
        "        plt.scatter(xd[:, 0].cpu(), xd[:,1].cpu(),c='deepskyblue',alpha=.5)\n",
        "        plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "        plt.title(strtitle)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "plt.plot(Glosses)\n",
        "plt.title('Generator loss')\n",
        "plt.show()\n",
        "\n",
        "# Save final generator for later use\n",
        "wgan = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "wgan.load_state_dict(G.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c9dbbce",
      "metadata": {
        "id": "4c9dbbce"
      },
      "source": [
        "## Sensitivy to hyper-parameters\n",
        "\n",
        "QUESTION: Repeat the WGAN learning experiment, by changing the parameters of the optimization (learning rates, number of updates of D and G, etc).\n",
        "\n",
        "From your experiments, can you draw a few recommendations for stable training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ab81c9",
      "metadata": {
        "id": "99ab81c9"
      },
      "outputs": [],
      "source": [
        "### ... ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765adf36",
      "metadata": {
        "id": "765adf36"
      },
      "source": [
        "## Train the generator only\n",
        "\n",
        "QUESTION: For a fixed discriminator, optimize only the generator only. Can you explain what is happening then?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3780f65c",
      "metadata": {
        "id": "3780f65c"
      },
      "outputs": [],
      "source": [
        "### ... ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa23a380",
      "metadata": {
        "id": "aa23a380"
      },
      "source": [
        "<br/><br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644cd14d",
      "metadata": {
        "id": "644cd14d"
      },
      "source": [
        "# BONUS Exercise (to do at home) : Learn a standard GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec43534b",
      "metadata": {
        "id": "ec43534b"
      },
      "source": [
        "## Define Loss for GAN training\n",
        "\n",
        "For stable GAN training uses the Binary cross-entropy loss which takes labeled data as input.\n",
        "\n",
        "The data points $y$ should correspond to values $\\approx 1$, whereas fake points $x$ should get values $\\approx 0$.\n",
        "\n",
        "We also introduce the sigmoid function. for displaying purpose: for GAN training, the discriminator values in $[0,1]$ will be obtained by applying after $D$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f177c994",
      "metadata": {
        "id": "f177c994"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "V1 = torch.full((n,1), 1. , dtype=torch.float, device=device)\n",
        "V0 = torch.full((b,1), 0. , dtype=torch.float, device=device)\n",
        "\n",
        "sig = nn.Sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c7b45b",
      "metadata": {
        "id": "61c7b45b"
      },
      "source": [
        "## Train the discriminator for a fixed generator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7cc9c4",
      "metadata": {
        "id": "2a7cc9c4"
      },
      "source": [
        "QUESTION: Complete the following code at the blocks ###...###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7c84b0",
      "metadata": {
        "scrolled": false,
        "id": "7f7c84b0"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
        "\n",
        "# parameters for discriminator optimization\n",
        "lrdisc = 0.002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "niterD=1000\n",
        "\n",
        "# Initialize generators and discriminators\n",
        "G = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "optimG = optim.Adam(G.parameters(), lr=lr)\n",
        "\n",
        "D = Discriminator(n_in=d, n_hid=10).to(device)\n",
        "optimD = optim.Adam(D.parameters(), lr=lrdisc, betas=(beta_1, 0.999))\n",
        "\n",
        "iter_display = 100  # display current configuration each iter_display iteration\n",
        "\n",
        "Dlosses = []\n",
        "\n",
        "for iter in range(0,niterD):\n",
        "\n",
        "    ### UPDATE OF D ###\n",
        "    ### ... ###\n",
        "\n",
        "\n",
        "    ### SAVE LOSS ###\n",
        "    Dlosst = Dloss.item()\n",
        "    Dlosses.append(-Dlosst)\n",
        "\n",
        "    if(iter%iter_display == 0):\n",
        "        print('[%d/%d], %f' % (iter, niterD, Dlosst))\n",
        "        Dxgrid = sig(D(xgrid)).detach().cpu().numpy().reshape(nr,nc)\n",
        "        x = G(z)\n",
        "        xd = x.detach().squeeze(1)\n",
        "        strtitle = 'Iter '+str(iter)\n",
        "        fig = plt.figure(dpi=100)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(Dxgrid,cmap = 'Oranges', extent=extent)  # discriminator\n",
        "        plt.scatter(xd[:, 0].cpu(), xd[:,1].cpu(),c='deepskyblue',alpha=.5)\n",
        "        plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "        plt.title(strtitle)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "### Plot the evolution of the discriminator loss ###\n",
        "plt.figure(dpi=100)\n",
        "plt.plot(Dlosses)\n",
        "plt.title('Discriminator loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "801b1d56",
      "metadata": {
        "id": "801b1d56"
      },
      "source": [
        "## Train both the Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6050b25c",
      "metadata": {
        "scrolled": false,
        "id": "6050b25c"
      },
      "outputs": [],
      "source": [
        "lr = 0.002   # learning rate for generator\n",
        "lrdisc = 0.002   # learning rate for generator\n",
        "\n",
        "## parameters for training\n",
        "n_epochs = 100\n",
        "niterD=100\n",
        "niterG=1\n",
        "\n",
        "torch.manual_seed(1)  # initialize random seed for reproducibility\n",
        "\n",
        "# Initialize generators and discriminators\n",
        "G = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "optimG = optim.Adam(G.parameters(), lr=lr)\n",
        "\n",
        "D = Discriminator(n_in=d, n_hid=10).to(device)\n",
        "optimD = optim.Adam(D.parameters(), lr=lrdisc, betas=(beta_1, beta_2))\n",
        "\n",
        "Glosses = []\n",
        "Dlosses = []\n",
        "\n",
        "iter_display = 10\n",
        "\n",
        "# Main loop\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    ############################\n",
        "    ### Train discriminator (niterD iterations)\n",
        "    ############################\n",
        "    for iter in range(0,niterD):\n",
        "        ### ... ###\n",
        "\n",
        "    ############################\n",
        "    ### Train generator (niterG iterations)\n",
        "    ############################\n",
        "    for iter in range(0,niterG):\n",
        "        ### ... ###\n",
        "\n",
        "\n",
        "    # Output training stats\n",
        "    print('[%d/%d] \\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
        "      % (epoch, n_epochs, Dloss.item(), Gloss.item()))\n",
        "    Glosses.append(Gloss.item())\n",
        "    Dlosses.append(-Dloss.item())\n",
        "\n",
        "\n",
        "    if(epoch % iter_display == 0):\n",
        "        Dxgrid = sig(D(xgrid)).detach().cpu().numpy().reshape(nr,nc)\n",
        "        z = torch.randn(b, 1, n_in, device=device)\n",
        "        x = G(z)\n",
        "        xd = x.detach().squeeze(1)\n",
        "        strtitle = 'Epoch '+str(epoch)\n",
        "        fig = plt.figure(dpi=100)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(Dxgrid,cmap = 'Oranges', extent=extent)  # discriminator\n",
        "        plt.scatter(xd[:, 0].cpu(), xd[:,1].cpu(),c='deepskyblue',alpha=.5)\n",
        "        plt.scatter(y[:, 0].cpu(), y[:,1].cpu(),c='navy')\n",
        "        plt.title(strtitle)\n",
        "        plt.show()\n",
        "\n",
        "# Save final generator for later use\n",
        "gan = Generator(n_in=n_in, n_out=d, n_hid=10, nlayers=3, device=device)\n",
        "gan.load_state_dict(G.state_dict())\n",
        "\n",
        "### Plot the evolution of the discriminator loss ###\n",
        "plt.figure(dpi=100)\n",
        "plt.plot(Glosses)\n",
        "plt.title('Generator loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}